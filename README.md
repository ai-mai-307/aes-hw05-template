# Описание задания

## Формулировка задания

В этом практическом задании вы соберёте и обучите **многослойный перцептрон (MLP)** для многоклассовой классификации на датасете **Fashion-MNIST** (10 классов) с использованием фреймворка **PyTorch**. 

Цель работы — освоить базовые принципы работы с нейронными сетями в PyTorch, научиться настраивать архитектуру сети и понять, как различные **функции активации** (sigmoid, tanh, ReLU) и **параметры обучения** влияют на качество модели.

---

## Постановка задачи

**Пишите весь код решения в блокноте `solution.ipynb`!**

### 1. Подготовка данных и создание DataLoader
- Загрузите датасет Fashion-MNIST с использованием `torchvision.datasets.FashionMNIST`
- Реализуйте преобразования данных: нормализацию и преобразование в тензоры
- Создайте DataLoader для тренировочной и валидационной выборок с заданным размером батча

### 2. Создание модели MLP с использованием PyTorch
Создайте класс `MLP`, наследуемый от `torch.nn.Module`:

```python
class MLP(nn.Module):
    def __init__(
        self,
        layers: list[int],              # например: [784, 100, 50, 10]
        activation: str = "relu",       # "sigmoid" | "tanh" | "relu"
        dropout_rate: float = 0.0,      # вероятность dropout
    ):
        super().__init__()
        # Реализация слоев и функции активации
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Прямой проход
```
### 3. Настройка обучения

1. Реализуйте цикл обучения с вычислением функции потерь (cross-entropy)
2. Используйте оптимизатор Adam или SGD
3. Добавьте возможность использования Dropout для регуляризации
4. Реализуйте валидацию после каждой тренировочной эпохи

### 4. Логирование и визуализация

1. Сохраняйте значения loss и accuracy для train и validation после каждой эпохи
2. Реализуйте построение графиков обучения (loss и accuracy curves)
3. После обучения визуализируйте кривые обучения

### 5. Проведение экспериментов

Проведите две серии экспериментов:

**Серия 1 (обязательные эксперименты):**

| Номер эксперимента | Конфигурация сети | Инициализация | Функция активации | Размер батча | Скорость обучения | Количество эпох |
| ----------------------------------- | --------------------------------- | -------------------------- | --------------------------------- | ----------------------- | --------------------------------- | ----------------------------- |
| 1                                   | [728, 100, 50, 10]                | random                     | sigmoid                           | 16                      | 0.1                               | 35                            |
| 2                                   | [728, 100, 50, 10]                | random                     | sigmoid                           | 32                      | 0.01                              | 15                            |
| 3                                   | [728, 392, 196, 98, 10]           | xavier                     | tanh                              | 32                      | 0.01                              | 15                            |
| 4                                   | [728, 392, 196, 98, 49, 25, 10]   | laiming                    | ReLU                              | 64                      | 0.01                              | 15                            |


**Вторую серию экспериментов** вы должны составить самостоятельно, чтобы проверить или опровергнуть следующие утверждения:

* выбор функции активации sigmoid в глубоких нейронных сетях негативно сказывается на обучении
* инициализация весов сети нулем или константой положительно сказывается на обучении
* инициализация весов сети Kaiming хороша для любой выбранной функции активации

## Критерии оценивания

### Обязательные условия

- Решение представлено в блокноте solution.ipynb;
- Блокнот имеет аккуратный вид (заголовки, пояснения, отсутствие лишнего кода и ошибок - выполнения);
- Использован PyTorch для реализации нейронной сети;
- Результаты экспериментов сведены в таблицу (очень предпочтительно Google Sheet);
- Отсутствует недобросовестное использование LLM;
- Зафиксирован seed для воспроизводимости экспериментов.

Работа, не соответствующая этим условиям, оценивается в 0 баллов. Допускается одна попытка исправления.

### Начисление баллов

- Подготовка данных и создание DataLoader — 2 балла: Данные корректно загружены и подготовлены, созданы DataLoader для тренировочной и валидационной выборок.

- Реализация модели MLP на PyTorch — 2 балла: Создан класс MLP с возможностью выбора архитектуры и функции активации, корректно реализован метод forward.

- Реализация цикла обучения и валидации — 2 балла: Реализован полный цикл обучения с вычислением loss, backward pass и обновлением весов, добавлена валидация после каждой эпохи.

- Логирование метрик и визуализация — 2 балла: Сохранены метрики обучения (loss, accuracy), построены информативные графики обучения для train и validation.

- Проведение экспериментов и анализ — 2 балла: Проведены все обязательные эксперименты и проверены гипотезы, даны осмысленные выводы с ссылками на полученные результаты.

### Снятие баллов

- за неправильную подготовку данных;
- за ошибки в реализации модели или цикла обучения;
- за неполное логирование метрик;
- за неполное проведение экспериментов;
- за отсутствие анализа результатов.